{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled18.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMMS7u/ccNsMAt0RnI/RNmJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"h8AwTufY59yJ","executionInfo":{"status":"ok","timestamp":1638092543148,"user_tz":-120,"elapsed":691,"user":{"displayName":"Faustin Nzitonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03681400882233025560"}}},"source":["import torch\n","import pandas as pd\n","from collections import Counter\n","import argparse\n","import numpy as np\n","from torch import nn, optim\n","from torch.utils.data import DataLoader"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PV1IF5t6AFO","executionInfo":{"status":"ok","timestamp":1638094006449,"user_tz":-120,"elapsed":668,"user":{"displayName":"Faustin Nzitonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03681400882233025560"}}},"source":["class Model(nn.Module):\n","    def __init__(self, dataset):\n","        super(Model, self).__init__()\n","        self.lstm_size = 128\n","        self.embedding_dim = 128\n","        self.num_layers = 3\n","\n","        n_vocab = len(dataset.uniq_words)\n","        self.embedding = nn.Embedding(\n","            num_embeddings=n_vocab,\n","            embedding_dim=self.embedding_dim,\n","        )\n","        self.lstm = nn.LSTM(\n","            input_size=self.lstm_size,\n","            hidden_size=self.lstm_size,\n","            num_layers=self.num_layers,\n","            dropout=0.2,\n","        )\n","        self.fc = nn.Linear(self.lstm_size, n_vocab)\n","\n","    def forward(self, x, prev_state):\n","        embed = self.embedding(x)\n","        output, state = self.lstm(embed, prev_state)\n","        logits = self.fc(output)\n","        print(logits.shape)\n","        return logits, state\n","\n","    def init_state(self, sequence_length):\n","        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n","                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbuj71Gh6Sxm","executionInfo":{"status":"ok","timestamp":1638094009668,"user_tz":-120,"elapsed":511,"user":{"displayName":"Faustin Nzitonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03681400882233025560"}}},"source":["# !wget https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"HIZYXi__6aNr","executionInfo":{"status":"ok","timestamp":1638094011649,"user_tz":-120,"elapsed":13,"user":{"displayName":"Faustin Nzitonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03681400882233025560"}}},"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(\n","        self,\n","        sequence_length,\n","    ):\n","        self.sequence_length = sequence_length\n","        self.words = self.load_words()\n","        self.uniq_words = self.get_uniq_words()\n","\n","        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n","        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n","\n","        self.words_indexes = [self.word_to_index[w] for w in self.words]\n","\n","    def load_words(self):\n","        train_df = pd.read_csv('reddit-cleanjokes.csv')\n","        text = train_df['Joke'].str.cat(sep=' ')\n","        return text.split(' ')\n","\n","    def get_uniq_words(self):\n","        word_counts = Counter(self.words)\n","        return sorted(word_counts, key=word_counts.get, reverse=True)\n","\n","    def __len__(self):\n","        return len(self.words_indexes) - self.sequence_length\n","\n","    def __getitem__(self, index):\n","        return (\n","            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n","            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n","        )"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"6rMma00c6n5c","executionInfo":{"status":"ok","timestamp":1638094159728,"user_tz":-120,"elapsed":589,"user":{"displayName":"Faustin Nzitonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03681400882233025560"}}},"source":["def train(dataset, model, batch_size,sequence_length,max_epochs):\n","    model.train()\n","\n","    dataloader = DataLoader(dataset, batch_size=batch_size)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    for epoch in range(max_epochs):\n","        state_h, state_c = model.init_state(sequence_length)\n","\n","        for batch, (x, y) in enumerate(dataloader):\n","            optimizer.zero_grad()\n","\n","            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n","            print(\"-----------------\")\n","            print(y_pred.shape)\n","            print(y.shape)\n","            print(y_pred.transpose(1, 2).shape)\n","            print(\"------------\")\n","            loss = criterion(y_pred.transpose(1, 2), y)\n","\n","            state_h = state_h.detach()\n","            state_c = state_c.detach()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"owGk_exC7Wmz","executionInfo":{"status":"ok","timestamp":1638094161492,"user_tz":-120,"elapsed":9,"user":{"displayName":"Faustin Nzitonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03681400882233025560"}}},"source":["def predict(dataset, model, text, next_words=100):\n","    model.eval()\n","\n","    words = text.split(' ')\n","    state_h, state_c = model.init_state(len(words))\n","\n","    for i in range(0, next_words):\n","        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n","        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n","\n","        last_word_logits = y_pred[0][-1]\n","        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n","        word_index = np.random.choice(len(last_word_logits), p=p)\n","        words.append(dataset.index_to_word[word_index])\n","\n","    return words"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mEPiXxyQ7bUh","executionInfo":{"status":"error","timestamp":1638094167606,"user_tz":-120,"elapsed":4254,"user":{"displayName":"Faustin Nzitonda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03681400882233025560"}},"outputId":"4569b93d-27f9-4f14-8af0-310efe010b17"},"source":["# parser = argparse.ArgumentParser()\n","# parser.add_argument('--max-epochs', type=int, default=10)\n","# parser.add_argument('--batch-size', type=int, default=256)\n","# parser.add_argument('--sequence-length', type=int, default=4)\n","# args = parser.parse_args()\n","\n","batch_size,sequence_length,max_epochs = 256,4,10\n","\n","dataset = Dataset(sequence_length)\n","model = Model(dataset)\n","\n","train(dataset, model, batch_size, sequence_length, max_epochs)\n","print(predict(dataset, model, text='Knock knock. Whos there?'))"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([256, 4, 6925])\n","-----------------\n","torch.Size([256, 4, 6925])\n","torch.Size([256, 4])\n","torch.Size([256, 6925, 4])\n","------------\n","{'epoch': 0, 'batch': 0, 'loss': 8.848093032836914}\n","torch.Size([256, 4, 6925])\n","-----------------\n","torch.Size([256, 4, 6925])\n","torch.Size([256, 4])\n","torch.Size([256, 6925, 4])\n","------------\n","{'epoch': 0, 'batch': 1, 'loss': 8.84752082824707}\n","torch.Size([256, 4, 6925])\n","-----------------\n","torch.Size([256, 4, 6925])\n","torch.Size([256, 4])\n","torch.Size([256, 6925, 4])\n","------------\n","{'epoch': 0, 'batch': 2, 'loss': 8.839971542358398}\n","torch.Size([256, 4, 6925])\n","-----------------\n","torch.Size([256, 4, 6925])\n","torch.Size([256, 4])\n","torch.Size([256, 6925, 4])\n","------------\n","{'epoch': 0, 'batch': 3, 'loss': 8.82830810546875}\n","torch.Size([256, 4, 6925])\n","-----------------\n","torch.Size([256, 4, 6925])\n","torch.Size([256, 4])\n","torch.Size([256, 6925, 4])\n","------------\n","{'epoch': 0, 'batch': 4, 'loss': 8.823491096496582}\n","torch.Size([256, 4, 6925])\n","-----------------\n","torch.Size([256, 4, 6925])\n","torch.Size([256, 4])\n","torch.Size([256, 6925, 4])\n","------------\n","{'epoch': 0, 'batch': 5, 'loss': 8.80438232421875}\n","torch.Size([256, 4, 6925])\n","-----------------\n","torch.Size([256, 4, 6925])\n","torch.Size([256, 4])\n","torch.Size([256, 6925, 4])\n","------------\n","{'epoch': 0, 'batch': 6, 'loss': 8.802789688110352}\n","torch.Size([256, 4, 6925])\n","-----------------\n","torch.Size([256, 4, 6925])\n","torch.Size([256, 4])\n","torch.Size([256, 6925, 4])\n","------------\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-55e17eea56d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Knock knock. Whos there?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-d5674a3b087f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, batch_size, sequence_length, max_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mstate_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"flx0_-qA7gCJ"},"source":[""],"execution_count":null,"outputs":[]}]}